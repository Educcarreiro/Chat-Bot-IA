{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ffad31-25e6-4128-abcc-323ef5d139e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Instalação necessaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec870ec-6f49-4210-a065-1e00bf315779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install langchain huggingface_hub tiktoken\n",
    "!pip -q install chromadb\n",
    "!pip -q install PyPDF2 pypdf sentence_transformers\n",
    "!pip -q install -U FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced47e2d-7cb0-407e-9b5e-c1414ad0c355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d854b55c-3531-4dfc-9ec4-e6660adfc402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e9c9a76-97b5-42e7-abd1-0d4e12f88d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1618ab33-cdc5-4221-ae27-77c0207dde2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be0190b3-c6ac-4ec2-bc35-625e6575c522",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95631780-4ffd-46be-a06e-1a42963b3fed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Importações "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "307968da-d26e-4523-90c0-9dcc7850047e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maria.carreiro/.conda/envs/eduarda2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import textwrap\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from typing import Any, List, Mapping, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b07e92-5eee-4079-8ce7-33360df959cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"/scratch/Llama-2-13b-chat-hf\"\n",
    "#Caminho do modelo que voce pretende utilizar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22d40eeb-bb89-44c6-9257-3790a7581836",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [04:00<00:00, 80.02s/it] \n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0764ed07-503a-431d-a217-9a61b47635cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, is_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "777dcb12-8e5b-474b-a9cc-46d690944659",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maria.carreiro/.conda/envs/eduarda2/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maria.carreiro/.conda/envs/eduarda2/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig.from_pretrained('/scratch/......')\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aef54297-6e3c-499b-909f-11a82cdcf503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d753dc6f-a917-4da6-9513-b10ec559d3a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaLLM(LLM):\n",
    "    model = model\n",
    "\n",
    "    temperature: float = 0.1\n",
    "\n",
    "    max_new_tokens: int = 350\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"llama\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        with torch.no_grad():\n",
    "#             inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "            generation_output = model.generate(\n",
    "\n",
    "                    input_ids=inputs.input_ids,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    temperature=self.temperature,\n",
    "                )\n",
    "            generated_text = tokenizer.decode(generation_output.sequences[0], skip_special_tokens=True)\n",
    "            generated_text = generated_text.split(\"[/INST]\")[1]\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dbfdd6b-d331-4e35-8bea-fbc529a81db3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Gerador de Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c994a6f3-e250-4c0d-8315-80134101c2d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#gera embeddings\n",
    "model_name = \"BAAI/bge-base-en\"\n",
    "encode_kwargs = {'normalize_embeddings': True} #Definir como 'True' para computar similaridade de cosseno\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_norm = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={'device': 'auto'},\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "\n",
    "embedding_function = model_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d092dba9-bfb5-4c06-b85c-c7f371677d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Carregamento de vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb68f52-e0cb-4f93-984a-fd4a5b6abcd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#chama o banco criado\n",
    "vet_db = Chroma(persist_directory=\"/scratch/caminho do seu banco de dados\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144564f-73ec-45d8-8b07-286a3bc5fa77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Retrieval-Augmented Generation (RAG), criação dos prompts e geração dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ae058-a1da-48eb-98fe-63aefc84a0e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Cria um Retriever\n",
    "retriever = vet_db.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621ecea5-19f6-453f-adfa-0ed4686a1f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cria o prompt de default\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "Você é um atendende de vendas. Suas respostas não devem incluir conteúdo prejudicial, antiético, racista, sexista, tóxico, perigoso ou ilegal.\n",
    "Suas respostas devem responder à pergunta apenas uma vez e não conter nenhum texto após a resposta ser feita.\n",
    "Se uma pergunta não fizer sentido ou não for  coerente, explique o porquê, em vez de responder algo incorreto. \n",
    "Se você não sabe a resposta a uma pergunta, não compartilhe informações falsas.\"\"\"\n",
    "#Coloque o prompt ideal de acordo com sua demanda \n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66850ad-5298-4c78-9f2e-28ed6717279d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b553645-1a80-49e1-88e5-75f311ff2800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac9d3b-d9dc-4828-9f55-024412035894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce304bb8-e9ad-4224-986a-e8581271daae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = LlamaLLM(\n",
    "    model= model,\n",
    "    temperature = 0.1,\n",
    "    max_new_tokens = 350,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e0dad8-5f21-4ad9-8145-947ba8074c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = get_prompt(instruction, sys_prompt)\n",
    "\n",
    "llama_prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce910a9-0b04-4173-bd98-8a337aedcf27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_type_kwargs = {\"prompt\": llama_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c55234-4413-4b2a-b737-f46c23b2104a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       retriever=retriever,\n",
    "                                       chain_type_kwargs=chain_type_kwargs,\n",
    "                                       return_source_documents=True\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b1fa97-e936-437d-8895-a989f9d8e4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Cita as fontes da resposta\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\n\\nFonte:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(\"IdDocumentoField: \", source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac2ee95-a746-4fa1-b78c-da1f836f9e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e8d32-1e26-462d-874c-83f660dd68de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Prompt Adequado\"\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d46397-b309-453f-aefc-f23da4fe84ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Prompt Adequado\"\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa56b9e-d8a4-44ed-a496-75cc588e2501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Prompt Adequado\"\n",
    "process_llm_response(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
